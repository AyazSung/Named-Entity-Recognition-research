{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:04.921283600Z",
     "start_time": "2023-10-05T19:24:03.279707700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harmense', 'Budy', 'Babitz', 'Birkenau', 'Raisko', 'Plawy', 'Golleschau', 'Jawischowitz', 'Chelmek', 'Monowitz Buna-Werke[6]', 'Eintrachthütte', 'Neu-Dachs', 'Fürstengrube', 'Janinagrube', 'Lagischa', 'Günthergrube', 'Gleiwitz I', 'Laurahütte', 'Blechhammer', 'Bobrek', 'Gleiwitz II', 'Sosnowitz II', 'Gleiwitz III', 'Hindenburg', 'Trzebinia', 'Tschechowitz I Bombensucherkommando[9]', 'Althammer', 'Bismarckhütte', 'Charlottengrube', 'Neustadt', 'Tschechowitz II Vacuum', 'Hubertshütte', 'Freudenthal', 'Lichtewerden', 'Sosnitz', 'Porombka', 'Altdorf', 'Radostowitz', 'Kobier', 'Brünn', 'Sosnowitz', 'Gleiwitz IV', 'Kattowitz', 'Bauzug']\n",
      "\n",
      "['Aachen', 'Berka', 'Berka', 'Stadtallendorf', 'Altenburg', 'Bad Arolsen', 'Aschersleben', 'Augustdorf', 'Bad Berka', 'Bad Gandersheim', 'Bad Salzungen', 'Bad Salzungen', 'Bergisch Gladbach', 'Berga', 'Berlstedt', 'Bernburg', 'Billroda', 'Blankenhain', 'Bochum', 'Bochum', 'Bochum', 'Böhlen', 'Braunschweig', 'Colditz', 'Crawinkel', 'Dessau', 'Dessau', 'Dortmund', 'Dortmund', 'Duisburg', 'Düsseldorf', 'Düsseldorf', 'Düsseldorf', 'Düsseldorf', 'Eisenach', 'Elsnig', 'Ohrdruf', 'Essen', 'Essen', 'Frohburg', 'Gelsenkirchen', 'Giessen', 'Goslar', 'Göttingen', 'Hadmersleben', 'Halberstadt', 'Halberstadt', 'Halberstadt', 'Halberstadt', 'Halle', 'Hessisch Lichtenau', 'Holzen', 'Jena', 'Kassel', 'Kassel', 'Cologne', 'Cologne', 'Cologne', 'Cologne', 'Kranichfeld', 'Bad Langensalza', 'Leipzig', 'Leipzig', 'Leipzig', 'Leipzig', 'Staßfurt', 'Prettin', 'Lippstadt', 'Lippstadt', 'near Mücheln', 'Magdeburg', 'Markkleeberg', 'Meuselwitz', 'Nordhausen', 'Mühlhausen', 'Mühlhausen', 'Niederorschel', 'Nordhausen', 'Kraftsdorf', 'Ohrdruf', 'Penig', 'Raguhn', 'Rothenburg', 'near Wurzbach', 'Schlieben', 'Schönebeck', 'Schönebeck', 'Schwerte', 'Sömmerda', 'Sonneberg', 'Staßfurt', 'Suhl', 'Bad Berka', 'Taucha', 'Tonndorf', 'Torgau', 'Elsteraue', 'Unna', 'Usingen', 'Weferlingen', 'Weimar', 'Wernigerode', 'Westeregeln', 'Witten-Annen', 'Wolfen']\n"
     ]
    }
   ],
   "source": [
    "#Code for gathering subcamps\n",
    "\n",
    "#import web scraping libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def grab_subcamps(url, start_row, cell, t_class=False):\n",
    "    '''\n",
    "    This function will grab table data from Wikipedia.\n",
    "    It allows you to grab specific rows and specific cells of data to cultivate lists of entities\n",
    "    \n",
    "    url       >> the Wikipedia url for grabbing data\n",
    "    start_row >> Where the data starts in the table\n",
    "    cell      >> The cell that you desire to grab from the table\n",
    "    t_class   >> Some Wikipedia tables have a specific class of table that you need to grab, i.e. \"wikipedia sortable\".\n",
    "    '''\n",
    "    #url of the page\n",
    "\n",
    "    #create the html object\n",
    "    s = requests.get(url).content\n",
    "\n",
    "    #set up the soup so that the html object can be parsed\n",
    "    soup = BeautifulSoup(s)\n",
    "\n",
    "    #grab the first table\n",
    "    if t_class:\n",
    "        table = soup.find(\"table\", {\"class\": \"wikitable sortable\"})\n",
    "    else:\n",
    "        table = soup.find(\"table\")\n",
    "\n",
    "    #create a blank list to append to for gathering camps\n",
    "    camps = []\n",
    "\n",
    "    #iterate over rows in table, beginning with row 3\n",
    "    for row in table.find_all(\"tr\")[start_row:]:\n",
    "        #one of the rows has only one cell, so we set up an index exception\n",
    "        try:\n",
    "            #grabs the 2nd cell in each row and cleans the data, splits off the cases of parentheses\n",
    "            #and grabs the first index from that split list\n",
    "            camp = row.find_all(\"td\")[cell].text.strip().split(\"(\")[0].split(\"/\")[0].strip()\n",
    "            camps.append(camp)\n",
    "        except:\n",
    "            IndexError\n",
    "    return camps\n",
    "\n",
    "\n",
    "#print off the subcamps of Auschwitz\n",
    "ausch_subcamps = grab_subcamps(\"https://en.wikipedia.org/wiki/List_of_subcamps_of_Auschwitz\", 2, 1)\n",
    "buch_subcamps = grab_subcamps(\"https://en.wikipedia.org/wiki/List_of_subcamps_of_Buchenwald\", 1, 1, t_class=True)\n",
    "\n",
    "print(ausch_subcamps)\n",
    "print(\"\")\n",
    "print(buch_subcamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alderney', 'Amersfoort', 'Auschwitz', 'Banjica', 'Bełżec', 'Bergen-Belsen,', 'Bernburg', 'Bogdanovka', 'Bolzano', 'Bor', 'Breendonk', 'Breitenau', 'Buchenwald,', 'Chełmno', 'Dachau', 'Drancy', 'Falstad', 'Flossenbürg', 'Fort VII', 'Fossoli', 'Grini', 'Gross-Rosen', 'Herzogenbusch', 'Hinzert', 'Janowska', 'Jasenovac', 'Kaiserwald', 'Kaunas', 'Kemna', 'Klooga', 'Le Vernet', 'Majdanek', 'Malchow', 'Maly Trostenets', 'Mechelen', 'Mittelbau-Dora', 'Natzweiler-Struthof', 'Neuengamme', 'Niederhagen', 'Oberer Kuhberg', 'Oranienburg', 'Osthofen', 'Płaszów', 'Ravensbruck', 'Risiera di San Sabba', 'Sachsenhausen', 'Sajmište', 'Salaspils', 'Sobibór', 'Soldau', 'Stutthof', 'Theresienstadt,', 'Trawniki', 'Treblinka', 'Vaivara']\n"
     ]
    }
   ],
   "source": [
    "ushmm_camps = ['Alderney', 'Amersfoort', 'Auschwitz', 'Banjica', 'Bełżec', 'Bergen-Belsen,', 'Bernburg', 'Bogdanovka',\n",
    "               'Bolzano', 'Bor', 'Breendonk',\n",
    "               'Breitenau', 'Buchenwald,', 'Chełmno', 'Dachau', 'Drancy', 'Falstad', 'Flossenbürg', 'Fort VII',\n",
    "               'Fossoli', 'Grini', 'Gross-Rosen',\n",
    "               'Herzogenbusch', 'Hinzert', 'Janowska', 'Jasenovac', 'Kaiserwald', 'Kaunas', 'Kemna', 'Klooga',\n",
    "               'Le Vernet', 'Majdanek', 'Malchow',\n",
    "               'Maly Trostenets', 'Mechelen', 'Mittelbau-Dora', 'Natzweiler-Struthof', 'Neuengamme', 'Niederhagen',\n",
    "               'Oberer Kuhberg', 'Oranienburg',\n",
    "               'Osthofen', 'Płaszów', 'Ravensbruck', 'Risiera di San Sabba', 'Sachsenhausen', 'Sajmište', 'Salaspils',\n",
    "               'Sobibór', 'Soldau', 'Stutthof',\n",
    "               'Theresienstadt,', 'Trawniki', 'Treblinka', 'Vaivara']\n",
    "print(ushmm_camps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:04.941845800Z",
     "start_time": "2023-10-05T19:24:04.918241500Z"
    }
   },
   "id": "c7385a063d4cb92"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alderney', 'Amersfoort', 'Auschwitz', 'Banjica', 'Belzec', 'Bergen-Belsen,', 'Bernburg', 'Bełżec', 'Bogdanovka', 'Bolzano', 'Bor', 'Breendonk', 'Breitenau', 'Buchenwald,', 'Chelmno', 'Chełmno', 'Dachau', 'Drancy', 'Falstad', 'Flossenburg', 'Flossenbürg', 'Fort VII', 'Fossoli', 'Grini', 'Gross-Rosen', 'Herzogenbusch', 'Hinzert', 'Janowska', 'Jasenovac', 'Kaiserwald', 'Kaunas', 'Kemna', 'Klooga', 'Le Vernet', 'Majdanek', 'Malchow', 'Maly Trostenets', 'Mechelen', 'Mittelbau-Dora', 'Natzweiler-Struthof', 'Neuengamme', 'Niederhagen', 'Oberer Kuhberg', 'Oranienburg', 'Osthofen', 'Plaszow', 'Płaszów', 'Ravensbruck', 'Risiera di San Sabba', 'Sachsenhausen', 'Sajmište', 'Salaspils', 'Sobibor', 'Sobibór', 'Soldau', 'Stutthof', 'Theresienstadt,', 'Trawniki', 'Treblinka', 'Vaivara']\n"
     ]
    }
   ],
   "source": [
    "def remove_accents(text):\n",
    "    #Polish letters\n",
    "    letters = {\n",
    "        'ł': 'l', 'ą': 'a', 'ń': 'n', 'ć': 'c', 'ó': 'o', 'ę': 'e', 'ś': 's', 'ź': 'z', 'ż': 'z',\n",
    "        'Ł': 'L', 'Ą': 'A', 'Ń': 'N', 'Ć': 'C', 'Ó': 'O', 'Ę': 'E', 'Ś': 'S', 'Ź': 'Z', 'Ż': 'Z',\n",
    "\n",
    "        #Accent Vowels\n",
    "        \"à\": \"a\", \"á\": \"a\", \"â\": \"a\", \"ã\": \"a\", \"ä\": \"a\", \"å\": \"a\", \"æ\": \"ae\",\n",
    "        \"À\": \"A\", \"Á\": \"A\", \"Â\": \"A\", \"Ã\": \"A\", \"Ä\": \"A\", \"Å\": \"A\", \"Æ\": \"ae\",\n",
    "\n",
    "        \"è\": \"e\", \"é\": \"e\", \"ê\": \"e\", \"ë\": \"e\",\n",
    "        \"È\": \"E\", \"É\": \"E\", \"Ê\": \"E\", \"Ë\": \"E\",\n",
    "\n",
    "        \"ì\": \"i\", \"í\": \"i\", \"î\": \"i\", \"ï\": \"i\",\n",
    "        \"Ì\": \"I\", \"Í\": \"I\", \"Î\": \"I\", \"Ï\": \"I\",\n",
    "\n",
    "        \"ò\": \"o\", \"ó\": \"o\", \"ô\": \"o\", \"õ\": \"o\", \"ö\": \"o\", \"ø\": \"o\",\n",
    "        \"Ò\": \"O\", \"Ó\": \"O\", \"Ô\": \"O\", \"Õ\": \"O\", \"Ö\": \"O\", \"Ø\": \"O\",\n",
    "\n",
    "        \"ù\": \"u\", \"ú\": \"u\", \"û\": \"u\", \"ü\": \"u\",\n",
    "        \"Ù\": \"U\", \"Ú\": \"U\", \"Û\": \"U\", \"Ü\": \"U\",\n",
    "\n",
    "        \"ý\": \"y\", \"ÿ\": \"y\",\n",
    "        \"Ý\": \"Y\", \"Ÿ\": \"Y\",\n",
    "\n",
    "        #Accent Cononants\n",
    "        \"ç\": \"c\", \"Ç\": \"C\",\n",
    "        \"ß\": \"ss\"\n",
    "    }\n",
    "\n",
    "    trans = str.maketrans(letters)\n",
    "    result = text.translate(trans)\n",
    "    return result\n",
    "\n",
    "\n",
    "final = []\n",
    "for camp in ushmm_camps:\n",
    "    final.append(camp)\n",
    "    final.append(remove_accents(camp))\n",
    "\n",
    "#Delete all duplicates\n",
    "ushmm_camps = list(set(final))\n",
    "ushmm_camps.sort()\n",
    "print(ushmm_camps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:05.098118Z",
     "start_time": "2023-10-05T19:24:04.940841400Z"
    }
   },
   "id": "8f456884259e2a50"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "sent1 = \"My name is William Mattingly.\"\n",
    "sent2 = \"My name is William Mattingly\"\n",
    "sent3 = \"my name is william mattingly\"\n",
    "sent4 = \"MY NAME IS WILLIAM MATTINGLY\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:05.099217Z",
     "start_time": "2023-10-05T19:24:04.967914700Z"
    }
   },
   "id": "1ecf8366ceb77cea"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IS', 'MATTINGLY', 'MY', 'Mattingly', 'Mattingly.', 'My', 'NAME', 'WILLIAM', 'William', 'is', 'mattingly', 'my', 'name', 'william']\n"
     ]
    }
   ],
   "source": [
    "sents = [sent1, sent2, sent3, sent4]\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for sent in sents:\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "\n",
    "no_duplicates = list(set(all_words))\n",
    "no_duplicates.sort()\n",
    "print (no_duplicates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:05.101212800Z",
     "start_time": "2023-10-05T19:24:04.992002Z"
    }
   },
   "id": "8624679012f081e9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is william mattingly\n",
      "my name is william mattingly\n",
      "my name is william mattingly\n",
      "my name is william mattingly\n",
      "['is', 'mattingly', 'my', 'name', 'william']\n"
     ]
    }
   ],
   "source": [
    "sents = [sent1, sent2, sent3, sent4]\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for sent in sents:\n",
    "    #New line in which we lowercase and remove the period punctuation.\n",
    "    sent = sent.lower().replace(\".\", \"\")\n",
    "    print (sent)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "\n",
    "no_duplicates = list(set(all_words))\n",
    "no_duplicates.sort()\n",
    "print (no_duplicates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:05.121303700Z",
     "start_time": "2023-10-05T19:24:05.013432200Z"
    }
   },
   "id": "5a7d72f2b303ebbd"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ball be his\n",
      "the ball be his\n",
      "['ball', 'be', 'his', 'the']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sent1 = \"The ball is his.\"\n",
    "sent2 = \"THE BALL WAS HIS.\"\n",
    "\n",
    "sents = [sent1, sent2]\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for sent in sents:\n",
    "    sent = sent.lower().replace(\".\", \"\")\n",
    "    doc = nlp(sent)\n",
    "    new = []\n",
    "    for token in doc:\n",
    "        if \"-\" not in token.lemma_:\n",
    "            new.append(token.lemma_)\n",
    "        else:\n",
    "            new.append(token.text)\n",
    "    sent = \" \".join(new)\n",
    "    print (sent)\n",
    "    words = sent.split()\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "\n",
    "no_duplicates = list(set(all_words))\n",
    "no_duplicates.sort()\n",
    "print (no_duplicates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:21.598906700Z",
     "start_time": "2023-10-05T19:24:05.029083400Z"
    }
   },
   "id": "458eba0002559709"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import spacy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:21.659098900Z",
     "start_time": "2023-10-05T19:24:21.601924100Z"
    }
   },
   "id": "51b678a2a11a9b5b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# main_nlp = spacy.blank(\"en\")\n",
    "# \n",
    "# en_ner = spacy.load(\"en_core_web_sm\").get_pipe(\"ner\")\n",
    "# de_ner = spacy.load(\"de_core_news_sm\").get_pipe(\"ner\")\n",
    "# \n",
    "# main_nlp.add_pipe(en_ner, name=\"en_ner\")\n",
    "# main_nlp.add_pipe(de_ner, name=\"de_ner\")\n",
    "# \n",
    "# for pipe in main_nlp.pipeline:\n",
    "#     print (pipe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:21.675136500Z",
     "start_time": "2023-10-05T19:24:21.616513100Z"
    }
   },
   "id": "ffda84b25e7410ce"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_transformers\n",
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "import re\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:21.678640100Z",
     "start_time": "2023-10-05T19:24:21.633549500Z"
    }
   },
   "id": "56a226e0fd2f3ddf"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.481731300Z",
     "start_time": "2023-10-05T19:24:21.647062Z"
    }
   },
   "id": "58344be2242308a1"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_streets(doc)>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streets_pattern = r\"([A-Z][a-z]*(strasse|straße|straat)\\b|([A-Z][a-z]* (Street|St|Boulevard|Blvd|Avenue|Ave|Road|Rd|Lane|Ln|Place|Pl)(\\.)*))\"\n",
    "@Language.component(\"find_streets\")\n",
    "def find_streets(doc):\n",
    "    text = doc.text\n",
    "    camp_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(streets_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            camp_ents.append((span.start, span.end, span.text))\n",
    "    for ent in camp_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"STREET\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)\n",
    "nlp.add_pipe(\"find_streets\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.580287Z",
     "start_time": "2023-10-05T19:24:33.514121700Z"
    }
   },
   "id": "92d2fe49ae69b092"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_ships(doc)>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ships_pattern = r\"((S.S. |SS |The )*(Lieutenant Colonel James Barker|General Hosey|Pan Crescent|Marilyn Marlene|Winnipeg|Ile de France|Scythia|Aquitania|Empress of Britain|General A. W. Greely|General J. H. McRae|Empress of Scotland|General T. H. Bliss|New Amsterdam|Niagara|Henry Gibbs|Serpa Pinto|Mauretania|Cabo de Hornos|Julius Caesar|Ben Hecht|Sțrumah|Strumah|General Harry Taylor|General W.P. Richardson|Marine Jumper|Simon Bolivar|Pan York|Mauretania|Orduña|Wilhelm Gustloff|Orduna|General W.H. Gordon|Rakuyō Maru|Rakuyo Maru|Mouzinho|Saturnia|St. Louis|Saint Louis|Nyassa|Simon Bolivar|Queen Elizabeth|Exodus 1947|Dunera|Cap Arcona|Ernie Pyle|Hayim Arlozorov|Patria))\"\n",
    "@Language.component(\"find_ships\")\n",
    "def find_ships(doc):\n",
    "    text = doc.text\n",
    "    new_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    nautical = [\"ship\", \"boat\", \"sail\", \"captain\", \"sea\", \"harbor\", \"aboard\", \"admiral\", \"liner\"]\n",
    "    for match in re.finditer(ships_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        context = text[start-100:end+100]\n",
    "        if any(term in context.lower() for term in nautical):\n",
    "            if span is not None:\n",
    "                new_ents.append((span.start, span.end, span.text))\n",
    "            else:\n",
    "                span = doc.char_span(start, end-1)\n",
    "                if span is not None:\n",
    "                    new_ents.append((span.start, span.end-1, span.text))\n",
    "    for ent in new_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"SHIP\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return doc\n",
    "nlp.add_pipe(\"find_ships\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.674567Z",
     "start_time": "2023-10-05T19:24:33.588803800Z"
    }
   },
   "id": "91e5f375c6dfae07"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_ghettos(doc)>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghetto_pattern = r\"(Anykščiai|Anyksciai|Arad|Ashmiany|Babruĭsk|Babruisk|Balassagyarmat|Baranavichy|Barysaŭ|Barysau|Będzin|Bedzin|Bełżyce|Belzyce|Berdychiv|Berehove|Berestechko|Berezdiv|Berezhany|Berezne|Bershad'|Biała Podlaska|Biala Podlaska|Birkenau|Biała Rawska|Białystok|Bialystok|Biaroza|Bibrka|Bielsko-Biała|Biržai|Bitola|Blazhiv|Bobowa|Bochnia|Bolekhiv|Borshchuv|Boryslav|Boskovice|Brańsk|Bratslav|Brody|Brzesko|Buczacz|Budapest|Bus'k|Bychawa|Chashniki|Chrzanów|Chrzanow|Ciechanów|Ciechanow|Cieszanów|Cristuru Secuiesc|Czernowitz|Częstochowa|Czortków|Dąbrowa Górnicza|Dąbrowa Tarnowska|Damashėvichy|Daugavpils|Dokshytsy|Dombóvár|Dombrowa|Drohobycz|Drzewica|Dubrovytsia|Dzialoszyce|Dziarechyn|Dziatlava|Glebokie|Gol'shany|Góra Kalwaria|Gorodnaia|Gostynin|Gyöngyös|Hajdúszoboszló|Halushchyntsi|Halych|Hantsavichy|Haradnaia|Hatvan|Hlusk|Hlyniany|Homel'|Horodenka|Horokhiv|Hradzianka|Hrodna|Hvizdets'|Iaktoriv|Izbica Lubelska|Józefów|Kalisz|Kałuszyn|Kam'iane Pole|Kamin'-Kashyrs'kyĭ|Katowice|Kecskemét|Kelme|Kharkiv|Khmel'nyts'ka oblast'|Khmel'nyts'kyĭ|Khust|Kielce|Kisvárda|Kletsk|Kobryn|Kolbuszowa|Kolozsvár|Komarów-Osada|Kopychyntsi|Korets'|Košice|Kőszeg|Kovel'|Kozienice|Kraków|Kraśnik|Kretinga|Krośniewice|Krymne|Kryzhopil'|Kul'chyny|Kunhegyes|Kutno|Kysylyn|Ladyzhyn|Lakhva|Lask|Lęczyca|Lesko|Lida|Liepāja|Lipinki|Lithakia|Litin|Litzmannstadt|Liubavichi|Łomża|Lubaczów|Lubartów|Lublin|Łuck|Lwów|Lyubcha|Mahiliou|Maków Mazowiecki|Marcinkonys|Matejovce nad Hornádom|Mátészalka|Miechów|Międzyrzec Podlaski|Minsk|Mir|Miskolc|Modliborzyce|Mogilev|Monastyrok|Monor|Munkács|Nadvirna|Nagyvárad|Navahrudak|Novomyrhorod|Nowy Sącz|Nyíregyháza|Odessa|Oleyëvo-Korolëvka|Opatów|Opoczno|Opole|Opole Lubelskie|Orla|Orsha|Ostroh|Ostrowiec Świętokrzyski|Otwock|Ozarintsy|Ozorków|Pabianice|Papul|Parichi|Pechera|Pinsk|Piotrków Trybunalski|Płaszów|Płock|Plońsk|Praszka|Prienai|Prużana|Pruzhany|Przemyśl|Pułtusk|Radom|Radomyśl Wielki|Radun'|Rava-Rus'ka|Rawa Mazowiecka|Reghin|Ribnița|Riga|Rohatyn|Romanove Selo|Rozhyshche|Rudky|Rudnik nad Sanem|Rzeszów|Saharna|Šahy|Salgótarján|Sarny|Sátoraljaújhely|Schwientochlowitz|Senkevychivka|Sernyky|Sharhorod|Shchyrets'|Shepetivka|Shpola|Shumilino|Šiauliai|Siedlce|Siedliszcze|Sieradz|Sighetu Marmației|Skalat|Slobodka|Slonim|Slutsk|Smolensk|Sokołów Podlaski|Sokyrnytsia|Solotvyno|Soroca|Sosnowiec|Stalovichy|Stanislav|Stara Mohylʹnytsia|Starachowice|Starokostiantyniv|Stary Sącz|Stepan'|Stoczek Lukowski|Stolbëisy|Stolin|Sucha|Suchowola|Surazh|Švenčionys|Szarvas|Szczebrzeszyn|Szeged|Szolnok|Tarnogród|Tarnów|Telšiai|Terebovlia|Ternopol|Theresienstadt|Thessalonike|Timkovichi|Tlumach|Tolna|Tomaszów Mazowiecki|Torchyn|Trakai|Trebíč|Trnava|Tul'chyn|Tuliszków|Tyvriv|Uzda|Uzhhorod|Vác|Valozhyn|Velizh|Velykyĭ Bereznyĭ|Vilna|Vinnytsia|Vlonia|Volodymyr-Volyns'kyi|Vysokovskiy Rayon|Warka|Warsaw|Wisznice|Wrocław|Žagarė|Zamość|Zarichne|Zboriv|Zduńska Wola|Zhmerinka|Zhytomyr|Žiežmariai|Anyksciai|Arad|Ashmiany|Babruisk|Balassagyarmat|Baranavichy|Barysau|Bedzin|Bełzyce|Berdychiv|Berehove|Berestechko|Berezdiv|Berezhany|Berezne|Bershad'|Biała Podlaska|Biała Rawska|Białystok|Biaroza|Bibrka|Bielsko-Biała|Birzai|Bitola|Blazhiv|Bobowa|Bochnia|Bolekhiv|Borshchuv|Boryslav|Boskovice|Bransk|Bratslav|Brody|Brzesko|Buczacz|Budapest|Bus'k|Bychawa|Chashniki|Chrzanow|Ciechanow|Cieszanow|Cristuru Secuiesc|Czernowitz|Czestochowa|Czortkow|Dabrowa Gornicza|Dabrowa Tarnowska|Damashevichy|Daugavpils|Dokshytsy|Dombovar|Dombrowa|Drohobycz|Drzewica|Dubrovytsia|Dzialoszyce|Dziarechyn|Dziatlava|Glebokie|Gol'shany|Gora Kalwaria|Gorodnaia|Gostynin|Gyongyos|Hajduszoboszlo|Halushchyntsi|Halych|Hantsavichy|Haradnaia|Hatvan|Hlusk|Hlyniany|Homel'|Horodenka|Horokhiv|Hradzianka|Hrodna|Hvizdets'|Iaktoriv|Izbica Lubelska|Jozefow|Kalisz|Kałuszyn|Kam'iane Pole|Kamin'-Kashyrs'kyi|Katowice|Kecskemet|Kelme|Kharkiv|Khmel'nyts'ka oblast'|Khmel'nyts'kyi|Khust|Kielce|Kisvarda|Kletsk|Kobryn|Kolbuszowa|Kolozsvar|Komarow-Osada|Kopychyntsi|Korets'|Kosice|Koszeg|Kovel'|Kozienice|Krakow|Krasnik|Kretinga|Krosniewice|Krymne|Kryzhopil'|Kul'chyny|Kunhegyes|Kutno|Kysylyn|Ladyzhyn|Lakhva|Lask|Leczyca|Lesko|Lida|Liepaja|Lipinki|Lithakia|Litin|Litzmannstadt|Liubavichi|Łomza|Lubaczow|Lubartow|Lublin|Łuck|Lwow|Lyubcha|Mahiliou|Makow Mazowiecki|Marcinkonys|Matejovce nad Hornadom|Mateszalka|Miechow|Miedzyrzec Podlaski|Minsk|Mir|Miskolc|Modliborzyce|Mogilev|Monastyrok|Monor|Munkacs|Nadvirna|Nagyvarad|Navahrudak|Novomyrhorod|Nowy Sacz|Nyiregyhaza|Odessa|Oleyevo-Korolevka|Opatow|Opoczno|Opole|Opole Lubelskie|Orla|Orsha|Ostroh|Ostrowiec Swietokrzyski|Otwock|Ozarintsy|Ozorkow|Pabianice|Papul|Parichi|Pechera|Pinsk|Piotrkow Trybunalski|Płaszow|Płock|Plonsk|Praszka|Prienai|Pruzana|Pruzhany|Przemysl|Pułtusk|Radom|Radomysl Wielki|Radun'|Rava-Rus'ka|Rawa Mazowiecka|Reghin|Ribnita|Riga|Rohatyn|Romanove Selo|Rozhyshche|Rudky|Rudnik nad Sanem|Rzeszow|Saharna|Sahy|Salgotarjan|Sarny|Satoraljaujhely|Senkevychivka|Sernyky|Sharhorod|Shchyrets'|Shepetivka|Shpola|Shumilino|Siauliai|Siedlce|Siedliszcze|Sieradz|Sighetu Marmatiei|Skalat|Slobodka|Slonim|Slutsk|Smolensk|Sokołow Podlaski|Sokyrnytsia|Solotvyno|Soroca|Sosnowiec|Stalovichy|Stanislav|Stara Mohylʹnytsia|Starachowice|Starokostiantyniv|Stary Sacz|Stepan'|Stoczek Lukowski|Stolbeisy|Stolin|Sucha|Suchowola|Surazh|Svencionys|Szarvas|Szczebrzeszyn|Szeged|Szolnok|Tarnogrod|Tarnow|Telsiai|Terebovlia|Ternopol|Theresienstadt|Thessalonike|Timkovichi|Tlumach|Tolna|Tomaszow Mazowiecki|Torchyn|Trakai|Trebic|Trnava|Tul'chyn|Tuliszkow|Tyvriv|Uzda|Uzhhorod|Vac|Valozhyn|Velizh|Velykyi Bereznyi|Vilna|Vinnytsia|Vlonia|Volodymyr-Volyns'kyi|Vysokovskiy Rayon|Warka|Warsaw|Wisznice|Wrocław|Zagare|Zamosc|Zarichne|Zboriv|Zdunska Wola|Zhmerinka|Zhytomyr|Ziezmariai)\"\n",
    "@Language.component(\"find_ghettos\")\n",
    "def find_ghettos(doc):\n",
    "    text = doc.text\n",
    "    ghetto_ents = []\n",
    "    gpe_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(ghetto_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        context = text[start-25:end+25]\n",
    "        if \"ghetto\" in context.lower():\n",
    "            if span is not None:\n",
    "                ghetto_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "        else:\n",
    "            if span is not None:\n",
    "                gpe_ents.append((span.start, span.end, span.text))\n",
    "    for ent in ghetto_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"GHETTO\")\n",
    "        original_ents.append(per_ent)\n",
    "    for ent in gpe_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"GPE\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)\n",
    "nlp.add_pipe(\"find_ghettos\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.692334400Z",
     "start_time": "2023-10-05T19:24:33.631903600Z"
    }
   },
   "id": "7992ac4bf900e218"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_ghettos2(doc)>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_ghettos_pattern = r\"[A-Z]\\w+((-| )*[A-Z]\\w+)* (g|G)hetto\"\n",
    "@Language.component(\"find_ghettos2\")\n",
    "def find_ghettos2(doc):\n",
    "    fps = [\"That\", \"The\"]\n",
    "    text = doc.text\n",
    "    camp_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(second_ghettos_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end-7)\n",
    "        if span is not None and span.text not in fps:\n",
    "            if \"The \" in span.text:\n",
    "                camp_ents.append((span.start+1, span.end, span.text))\n",
    "            else:\n",
    "                camp_ents.append((span.start, span.end, span.text))\n",
    "    for ent in camp_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"GHETTO\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)\n",
    "nlp.add_pipe(\"find_ghettos2\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.807834Z",
     "start_time": "2023-10-05T19:24:33.661048200Z"
    }
   },
   "id": "f79cb14b88205747"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "people_pattern = r\"((((Mr|Mrs|Miss|Dr|Col|Adm|Lt|Cap|Cpt|Fr|Cl|Cln|Sgt)\\.)|(Frau|Herr|President|Rabbi|Queen|Prince|Princess|Pope|Father|Bishop|King|Cardinal|General|Liutenant|Colonel|Lieutenant Colonel|Private|Admiral|Captain|Sergeant|Sergeant First Class|Staff Sergeant|Sergeant Major|Corp Sergeant Major|Field Sergeant|Technical Sergeant|Corporal|Lance Corporal|Ensign|2nd Lieutenant|1st Lieutenant|Major|Hauptmann|Staff Captain|Oberst|Oberstlieutenant)) (?:[A-Z]\\w+[ -]?)+)(the [A-Z]\\w*|I\\w*|X\\w*|v\\w*)*\"\n",
    "@Language.component(\"find_people\")\n",
    "def find_people(doc):\n",
    "    text = doc.text\n",
    "    match_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(people_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            match_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "        else:\n",
    "            span = doc.char_span(start, end-1)\n",
    "            if span is not None:\n",
    "                match_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "    for ent in match_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"PERSON\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.849273Z",
     "start_time": "2023-10-05T19:24:33.694333600Z"
    }
   },
   "id": "978dfd656c67a448"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_people(doc)>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"find_people\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.878337500Z",
     "start_time": "2023-10-05T19:24:33.712391400Z"
    }
   },
   "id": "821b56994c1d5d7f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "spousal_pattern = r\"((Mr|Mrs|Miss|Dr)(\\.)* and (Mr|Mrs|Miss|Dr)(\\.)* (?:[A-Z]\\w+[ -]?)+)\"\n",
    "@Language.component(\"find_spousal\")\n",
    "def find_spousal(doc):\n",
    "    text = doc.text\n",
    "    new_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(spousal_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            new_ents.append((span.start, span.end, span.text))\n",
    "        else:\n",
    "            span = doc.char_span(start, end-1)\n",
    "            if span is not None:\n",
    "                new_ents.append((span.start, span.end-1, span.text))\n",
    "    for ent in new_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"SPOUSAL\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.879347400Z",
     "start_time": "2023-10-05T19:24:33.740123900Z"
    }
   },
   "id": "410d189db69969d9"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_spousal(doc)>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"find_spousal\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.882345200Z",
     "start_time": "2023-10-05T19:24:33.751662800Z"
    }
   },
   "id": "d239f09c4f0a93fd"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def subcamp_getter(hit):\n",
    "    hit = hit.text\n",
    "    df = pd.read_csv(\"../youtube/holocaust_ner_lessons/data/hgc_data.csv\")\n",
    "    subcamps = df.Main.tolist()\n",
    "    camps = df.SubcampMattingly.tolist()\n",
    "    i=0\n",
    "    potential = []\n",
    "    for c in camps:\n",
    "\n",
    "        try:\n",
    "            all_c = c.split(\"^\")\n",
    "            for c in all_c:\n",
    "                c = c.replace(\"\\(\", \"(\").replace(\"\\)\", \")\")\n",
    "                #                 if c == \"Buna-Monowitz (Auschwitz III)\":\n",
    "                #                     print (c)\n",
    "                if hit.strip() == c.strip():\n",
    "                    #                     print (hit, c)\n",
    "                    if subcamps[i] not in potential:\n",
    "                        potential.append(subcamps[i])\n",
    "        except:\n",
    "            AttributeError\n",
    "        i=i+1\n",
    "    return (potential)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.886352500Z",
     "start_time": "2023-10-05T19:24:33.792732Z"
    }
   },
   "id": "13faf9b65072fb84"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def date_open_getter(hit):\n",
    "    hit = hit.text\n",
    "    df = pd.read_csv(\"../youtube/holocaust_ner_lessons/data/hgc_data.csv\")\n",
    "    dates = df.Date_Open.tolist()\n",
    "    camps = df.SubcampMattingly.tolist()\n",
    "    i=0\n",
    "    potential = []\n",
    "    for c in camps:\n",
    "\n",
    "        try:\n",
    "            all_c = c.split(\"^\")\n",
    "            for c in all_c:\n",
    "                if hit == c:\n",
    "                    if dates[i] not in potential:\n",
    "                        potential.append(dates[i])\n",
    "        except:\n",
    "            AttributeError\n",
    "        i=i+1\n",
    "    return (potential)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.973408400Z",
     "start_time": "2023-10-05T19:24:33.822695300Z"
    }
   },
   "id": "93a6b6902a62cda5"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def date_closed_getter(hit):\n",
    "    hit = hit.text\n",
    "    df = pd.read_csv(\"../youtube/holocaust_ner_lessons/data/hgc_data.csv\")\n",
    "    dates = df.Date_Close.tolist()\n",
    "    camps = df.SubcampMattingly.tolist()\n",
    "    i=0\n",
    "    potential = []\n",
    "    for c in camps:\n",
    "        try:\n",
    "            all_c = c.split(\"^\")\n",
    "            for c in all_c:\n",
    "                if hit == c:\n",
    "                    if dates[i] not in potential:\n",
    "                        potential.append(dates[i])\n",
    "        except:\n",
    "            AttributeError\n",
    "        i=i+1\n",
    "    return (potential)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:33.993966500Z",
     "start_time": "2023-10-05T19:24:33.852276500Z"
    }
   },
   "id": "c178066d9fe5815c"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def latlong_getter(hit):\n",
    "    hit = hit.text\n",
    "    df = pd.read_csv(\"../youtube/holocaust_ner_lessons/data/hgc_data.csv\")\n",
    "    lats = df.LAT.tolist()\n",
    "    longs = df.LONG.tolist()\n",
    "    camps = df.SubcampMattingly.tolist()\n",
    "    i=0\n",
    "    potential = []\n",
    "    for c in camps:\n",
    "\n",
    "        try:\n",
    "            all_c = c.split(\"^\")\n",
    "            for c in all_c:\n",
    "                if hit == c:\n",
    "                    if lats[i] not in potential:\n",
    "                        potential.append((lats[i], longs[i]))\n",
    "        except:\n",
    "            AttributeError\n",
    "        i=i+1\n",
    "    return (potential)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:34.080564900Z",
     "start_time": "2023-10-05T19:24:33.877784800Z"
    }
   },
   "id": "edd341b547c436c0"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def hgc_id_getter(hit):\n",
    "    hit = hit.text\n",
    "    df = pd.read_csv(\"../youtube/holocaust_ner_lessons/data/hgc_data.csv\")\n",
    "    ids = df.HGC_ID.tolist()\n",
    "    camps = df.SubcampMattingly.tolist()\n",
    "    i=0\n",
    "    potential = []\n",
    "    for c in camps:\n",
    "\n",
    "        try:\n",
    "            all_c = c.split(\"^\")\n",
    "            for c in all_c:\n",
    "                if hit == c:\n",
    "                    if ids[i] not in potential:\n",
    "                        potential.append(ids[i])\n",
    "        except:\n",
    "            AttributeError\n",
    "        i=i+1\n",
    "    return (potential)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:34.081563700Z",
     "start_time": "2023-10-05T19:24:33.893881800Z"
    }
   },
   "id": "22731bd67f283303"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def camp_type_getter(hit):\n",
    "    hit = hit.text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:34.082566700Z",
     "start_time": "2023-10-05T19:24:33.906411700Z"
    }
   },
   "id": "bda361608e7a6e2"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../youtube/holocaust_ner_lessons/data/hgc_data.csv\")\n",
    "camps = df.SubcampMattingly.tolist()\n",
    "subcamps = df.Main.tolist()\n",
    "i=0\n",
    "final_camps = []\n",
    "for c in camps:\n",
    "    if c != \"nan\" and c != \"FALSE\":\n",
    "        if subcamps[i] != \"nan\" and subcamps[i] != \"FALSE\":\n",
    "            try:\n",
    "                if c.split()[0] != \"\":\n",
    "                    c=c.replace(\"*\", \"\")\n",
    "                    for item in c.split(\"^\"):\n",
    "                        final_camps.append(item.replace(\"(\", \"\\(\").replace(\")\", \"\\)\").strip())\n",
    "            except:\n",
    "                AttributeError\n",
    "    i=i+1\n",
    "\n",
    "final_camps.sort(key=len, reverse=True)\n",
    "final_list = \"|\".join(final_camps)\n",
    "strict_camps_pattern = r\"(\"+final_list+\")\"\n",
    "# print (strict_camps_pattern)\n",
    "@Language.component(\"find_camps_strict\")\n",
    "def find_camps_strict(doc):\n",
    "    text = doc.text\n",
    "    camp_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    context_terms = [\"camp\", \"concentration\", \"labor\", \"forced\", \"gas\", \"chamber\"]\n",
    "    for match in re.finditer(strict_camps_pattern, doc.text):\n",
    "        #         print (match)\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        context = text[start-100:end+100]\n",
    "        if any(term in context.lower() for term in context_terms):\n",
    "            if span is not None:\n",
    "                #                 print (span)\n",
    "                camp_ents.append((span.start, span.end, span.text))\n",
    "    for ent in camp_ents:\n",
    "        #         print (ent)\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"CAMP\")\n",
    "        per_ent.set_extension(\"subcamp\", getter=subcamp_getter, force=True)\n",
    "        per_ent.set_extension(\"date_open\", getter=date_open_getter, force=True)\n",
    "        per_ent.set_extension(\"date_closed\", getter=date_closed_getter, force=True)\n",
    "        per_ent.set_extension(\"latlong\", getter=latlong_getter, force=True)\n",
    "        per_ent.set_extension(\"hgc_id\", getter=hgc_id_getter, force=True)\n",
    "\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:24:34.126050600Z",
     "start_time": "2023-10-05T19:24:33.931631600Z"
    }
   },
   "id": "11976f5a729b09de"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "camps_pattern = r\"(Alderney|Amersfoort|Auschwitz|Banjica|Belzec|Bergen-Belsen|Bernburg|Bogdanovka|Bolzano|Bor|Breendonk|Breitenau|Buchenwald|Chelmno|Dachau|Drancy|Falstad|Flossenburg|Fort VII|Fossoli|Grini|Gross-Rosen|Herzogenbusch|Hinzert|Janowska|Jasenovac|Kaiserwald|Kaunas|Kemna|Klooga|Le Vernet|Majdanek|Malchow|Maly Trostenets|Mechelen|Mittelbau-Dora|Natzweiler-Struthof|Neuengamme|Niederhagen|Oberer Kuhberg|Oranienburg|Osthofen|Plaszow|Ravensbruck|Risiera di San Sabba|Sachsenhausen|Sajmište|Salaspils|Sobibor|Soldau|Stutthof|Theresienstadt|Trawniki|Treblinka|Vaivara)(-[A-Z]\\S+)*\"\n",
    "@Language.component(\"find_camps\")\n",
    "def find_camps(doc):\n",
    "    text = doc.text\n",
    "    camp_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(camps_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            camp_ents.append((span.start, span.end, span.text))\n",
    "    for ent in camp_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"CAMP\")\n",
    "        per_ent.set_extension(\"subcamp\", getter=subcamp_getter, force=True)\n",
    "        per_ent.set_extension(\"date_open\", getter=date_open_getter, force=True)\n",
    "        per_ent.set_extension(\"date_closed\", getter=date_closed_getter, force=True)\n",
    "        per_ent.set_extension(\"latlong\", getter=latlong_getter, force=True)\n",
    "        per_ent.set_extension(\"hgc_id\", getter=hgc_id_getter, force=True)\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:25:36.947722200Z",
     "start_time": "2023-10-05T19:25:36.889455900Z"
    }
   },
   "id": "bb36a5e841d629c1"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "second_camps_pattern = r\"[A-Z]\\w+((-| )*[A-Z]\\w+)* (c|C)oncentration (c|C)amp\"\n",
    "@Language.component(\"find_camps2\")\n",
    "def find_camps2(doc):\n",
    "    text = doc.text\n",
    "    camp_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(second_camps_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end-19)\n",
    "        if span is not None:\n",
    "            camp_ents.append((span.start, span.end, span.text))\n",
    "    for ent in camp_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"CAMP\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:25:48.839731200Z",
     "start_time": "2023-10-05T19:25:48.770305900Z"
    }
   },
   "id": "381a6d24006169df"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_camps2(doc)>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"find_camps_strict\", before=\"ner\")\n",
    "nlp.add_pipe(\"find_camps\", before=\"ner\")\n",
    "nlp.add_pipe(\"find_camps2\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:25:52.646115Z",
     "start_time": "2023-10-05T19:25:52.481423800Z"
    }
   },
   "id": "c5d64b3ce4dea9df"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "groups_pattern = r\"(Ethnikon Apeleutherotikon Metopon|Weisse Rose|Rote Kapelle|Affiche rouge|Edelweisspiraten|White Rose|Bielski|Nekamah|Voroshilov|OEuvre de secours aux enfants|Union des juifs pour la résistance et l'entraide|Zorin Unit|Komsomolski|Fareynikte|Korzh|Zhukov|Budenny|Parkhomenko|Sixième)((-)*[A-Z]\\S+)*( (Brigade|brothers|group))*\"\n",
    "@Language.component(\"find_groups\")\n",
    "def find_groups(doc):\n",
    "    text = doc.text\n",
    "    camp_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(groups_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            camp_ents.append((span.start, span.end, span.text))\n",
    "    for ent in camp_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"GROUP\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:26:03.397762500Z",
     "start_time": "2023-10-05T19:26:03.320883700Z"
    }
   },
   "id": "4f9ef8c50619ffb"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_groups(doc)>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"find_groups\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:26:07.634609300Z",
     "start_time": "2023-10-05T19:26:07.513317800Z"
    }
   },
   "id": "d010c82cd3de5e8e"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "city_pattern = r\"(?:[A-Z]\\w+[ -]?)+, (Germany|Poland|England|Russia|Italy|USA|U.S.A.|United States|United States of America|America|United Kingdom|France|Spain|Ukraine|Romania|Netherlands|Belgium|Greece|Portugal|Sweden|Hungary|Austria|Belarus|Serbia|Switzerland|Bulgaria|Denmark|Finland|Slovakia|Norway|Ireland|Croatia|Moldova|Bosnia|Albania|Estonia|Malta|Iceland|Andorra|Luxembourg|Montenegro|Macedonia|San Marino|Lichtenstein|Monaco)\"\n",
    "country_pattern = r\"(Germany|Poland|England|Russia|Italy|USA|U.S.A.|United States|United States of America|America|United Kingdom|France|Spain|Ukraine|Romania|Netherlands|Belgium|Greece|Portugal|Sweden|Hungary|Austria|Belarus|Serbia|Switzerland|Bulgaria|Denmark|Finland|Slovakia|Norway|Ireland|Croatia|Moldova|Bosnia|Albania|Estonia|Malta|Iceland|Andorra|Luxembourg|Montenegro|Macedonia|San Marino|Lichtenstein|Monaco)\"\n",
    "@Language.component(\"find_places\")\n",
    "def find_places(doc):\n",
    "    text = doc.text\n",
    "    new_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(city_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            new_ents.append((span.start, span.end, span.text))\n",
    "        else:\n",
    "            span = doc.char_span(start, end-1)\n",
    "            if span is not None:\n",
    "                new_ents.append((span.start, span.end-1, span.text))\n",
    "    for ent in new_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"GPE\")\n",
    "        if per_ent.text.split(\",\")[0] not in city_pattern:\n",
    "            original_ents.append(per_ent)\n",
    "\n",
    "    for match in re.finditer(country_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            new_ents.append((span.start, span.end, span.text))\n",
    "        else:\n",
    "            span = doc.char_span(start, end-1)\n",
    "            if span is not None:\n",
    "                new_ents.append((span.start, span.end-1, span.text))\n",
    "    for ent in new_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"GPE\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:26:15.984335400Z",
     "start_time": "2023-10-05T19:26:15.906649800Z"
    }
   },
   "id": "2c49185184f429be"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_places(doc)>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe(\"find_places\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:26:20.694761Z",
     "start_time": "2023-10-05T19:26:20.617596200Z"
    }
   },
   "id": "3b98a3d81f7e9050"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "<function __main__.find_geography(doc)>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_pattern = r\"([A-Z]\\w+) (River|Mountain|Mountains|Forest|Forests|Sea|Ocean)*\"\n",
    "river_pattern = \"(the|The) (Rhone|Volga|Danube|Ural|Dnieper|Don|Pechora|Kama|Oka|Belaya|Dniester|Rhine|Desna|Elbe|Donets|Vistula|Tagus|Daugava|Loire|Tisza|Ebro|Prut|Neman|Sava|Meuse|Kuban River|Douro|Mezen|Oder|Guadiana|Rhône|Kuma|Warta|Seine|Mureș|Northern Dvina|Vychegda|Drava|Po|Guadalquivir|Bolshoy Uzen|Siret|Maly Uzen|Terek|Olt|Vashka|Glomma|Garonne|Usa|Kemijoki|Great Morava|Moselle|Main 525|Torne|Dalälven|Inn|Maritsa|Marne|Neris|Júcar|Dordogne|Saône|Ume|Mur|Ångerman|Klarälven|Lule|Gauja|Weser|Kalix|Vindel River|Ljusnan|Indalsälven|Vltava|Ponoy|Ialomița|Onega|Somes|Struma|Adige|Skellefte|Tiber|Vah|Pite|Faxälven|Vardar|Shannon|Charente|Iskar|Tundzha|Ems|Tana|Scheldt|Timiș|Genil|Severn|Morava|Luga|Argeș|Ljungan|Minho|Venta|Thames|Drina|Jiu|Drin|Segura|Torne|Osam|Arda|Yantra|Kamchiya|Mesta)\"\n",
    "@Language.component(\"find_geography\")\n",
    "def find_geography(doc):\n",
    "    text = doc.text\n",
    "    river_ents = []\n",
    "    general_ents = []\n",
    "    original_ents = list(doc.ents)\n",
    "    for match in re.finditer(river_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            river_ents.append((span.start, span.end, span.text))\n",
    "    for match in re.finditer(general_pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            general_ents.append((span.start, span.end, span.text))\n",
    "\n",
    "        #     all_ents = river_ents+general_ents       \n",
    "    for ent in river_ents:\n",
    "        start, end, name = ent\n",
    "        per_ent = Span(doc, start, end, label=\"RIVER\")\n",
    "        original_ents.append(per_ent)\n",
    "\n",
    "    for ent in general_ents:\n",
    "        start, end, name = ent\n",
    "        if \"River\" in name:\n",
    "            per_ent = Span(doc, start, end, label=\"RIVER\")\n",
    "        elif \"Mountain\" in name:\n",
    "            per_ent = Span(doc, start, end, label=\"MOUNTAIN\")\n",
    "        elif \"Sea\" in name:\n",
    "            per_ent = Span(doc, start, end, label=\"SEA-OCEAN\")\n",
    "        elif \"Forest\" in name:\n",
    "            per_ent = Span(doc, start, end, label=\"FOREST\")\n",
    "        original_ents.append(per_ent)\n",
    "    filtered = filter_spans(original_ents)\n",
    "    doc.ents = filtered\n",
    "    return (doc)\n",
    "nlp.add_pipe(\"find_geography\", before=\"ner\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:26:26.138951300Z",
     "start_time": "2023-10-05T19:26:26.027459900Z"
    }
   },
   "id": "97162b0b953c3e5d"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This is \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Berlinstrasse\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET</span>\n</mark>\n, which is also known as \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Berlin Street\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET</span>\n</mark>\n or \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Berlin St.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">STREET</span>\n</mark>\n That Ghetto and The Ghetto. The \n<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Warsaw\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GHETTO</span>\n</mark>\n Ghetto</div></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import glob\n",
    "# from spacy import displacy\n",
    "# hits = []\n",
    "# files = glob.glob(\"data/new_ocr/*trs_en.txt\")\n",
    "# all_data = {}\n",
    "# for file in files[5:6]:\n",
    "#     all_hits = []\n",
    "#     with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         print (file)\n",
    "#         text = f.read().replace(\"(ph)\", \"\")\n",
    "#         while \"  \" in text:\n",
    "#             text =  text.replace(\"  \", \" \")\n",
    "#         doc = nlp(text)\n",
    "#         colors = {\"CAMP\": \"#FF5733\", \"GHETTO\": \"#1F9D12\", \"SHIP\": \"#557DB4\", \"SPOUSAL\": \"#55B489\", \"GPE\":\"#17B4C2\", \"RIVER\": \"#9017C2\", \"MOUNTAIN\": \"#878787\", \"SEA-OCEAN\": \"#0A6DF5\", \"FOREST\": \"#1F541D\"}\n",
    "#         options = {\"ents\": [\"CAMP\", \"GHETTO\", \"SHIP\", \"SPOUSAL\", \"GPE\", \"RIVER\", \"MOUNTAIN\", \"SEA-OCEAN\", \"FOREST\"], \"colors\":colors}\n",
    "#         displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "doc = nlp(\"This is Berlinstrasse, which is also known as Berlin Street or Berlin St. That Ghetto and The Ghetto. The Warsaw Ghetto\")\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T19:26:40.327636200Z",
     "start_time": "2023-10-05T19:26:37.248480700Z"
    }
   },
   "id": "236dd32d56490934"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b2df37647d8ba01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
